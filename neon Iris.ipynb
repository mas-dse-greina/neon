{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using sklearn's Iris Dataset with neon\n",
    "\n",
    "Tony Reina<br>\n",
    "28 JUNE 2017\n",
    "\n",
    "Here's an example of how we can load one of the standard [sklearn](http://scikit-learn.org/stable/index.html) datasets into a neon model. We'll be using the [iris dataset](http://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html), a classification model which tries to predict the type of iris flower species (Setosa, Versicolour, and Virginica) based on 4 continuous parameters: Sepal Length, Sepal Width, Petal Length and Petal Width. It is based on Ronald Fisher's 1936 paper describing [Linear Discriminant Analysis](https://en.wikipedia.org/wiki/Iris_flower_data_set). The dataset is now considered one of the gold standards at monitoring the performance of a new classification method.\n",
    "\n",
    "In this notebook, we'll walk through loading the data from sklearn into neon's ArrayIterator class and then passing that to a simple multi-layer perceptron model. We should get a misclassification rate of 2% to 8%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
    "\n",
    ">     http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "> Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the iris dataset from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris.data  \n",
    "Y = iris.target\n",
    "\n",
    "nClasses = len(iris.target_names)  # Setosa, Versicolour, and Virginica iris species"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use sklearn to split the data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33) # 66% training, 33% testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make sure that the features are scaled to mean of 0 and standard deviation of 1\n",
    "\n",
    "This is standard pre-processing for multi-layered perceptron inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scl = StandardScaler()\n",
    "\n",
    "X_train = scl.fit_transform(X_train)\n",
    "X_test = scl.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a backend for neon to use\n",
    "\n",
    "This sets up either our GPU or CPU connection to neon. If we don't start with this, then ArrayIterator won't execute.\n",
    "\n",
    "We're asking neon to use the cpu, but can change that to a gpu if it is avaliable. Batch size refers to how many data points are taken at a time. Here's a primer on [Gradient Descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent).\n",
    "\n",
    ">Technical note:  Your batch size must always be much less than the number of points in your data. So if you have 50 points, then set your batch size to something much less than 50. I'd suggest setting the batch size to no more than 10% of the number of data points. You can always just set your batch size to 1. In that case, you are no longer performing mini-batch gradient descent, but are performing the standard stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from neon.data import ArrayIterator\n",
    "from neon.backends import gen_backend\n",
    "\n",
    "be = gen_backend(backend='cpu', batch_size=X_train.shape[0]//10)  # Change to 'gpu' if you have gpu support "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's pass the data to neon\n",
    "\n",
    "We pass our data (both features and labels) into neon's ArrayIterator class.  By default, ArrayIterator one-hot encodes the labels (which saves us a step). Once we get our ArrayIterators, then we can pass them directly into neon models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_data = ArrayIterator(X=X_train, y=y_train, nclass=nClasses, make_onehot=True)\n",
    "testing_data = ArrayIterator(X=X_test, y=y_test, nclass=nClasses, make_onehot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am using this backend: <neon.backends.nervanacpu.NervanaCPU object at 0x7fa55f41b450>\n"
     ]
    }
   ],
   "source": [
    "print ('I am using this backend: {}'.format(be))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the neon libraries we need for this MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from neon.initializers import GlorotUniform, Gaussian \n",
    "from neon.layers import GeneralizedCost, Affine, Dropout\n",
    "from neon.models import Model \n",
    "from neon.optimizers import GradientDescentMomentum\n",
    "from neon.transforms import Softmax, CrossEntropyMulti, Rectlin, Tanh\n",
    "from neon.callbacks.callbacks import Callbacks \n",
    "from neon.transforms import Misclassification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the weights and bias variables\n",
    "\n",
    "We could use numbers from the Gaussian distribution ($\\mu=0, \\sigma=0.3$) to initialize the weights and bias terms for our regression model. However, we can also use other initializations like [GlorotUniform](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = GlorotUniform()    #Gaussian(loc=0, scale=0.3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a multi-layered perceptron (MLP) model\n",
    "\n",
    "We just use a simple Python list to add our different layers to the model. The nice thing is that we've already put our data into a neon ArrayIterator. That means the model will automatically know how to handle the input layer.\n",
    "\n",
    "I've just thrown together a model haphazardly. In this model, the input layer feeds into a 4-neuron rectified linear unit affine layer. That feeds into an 8 neuron hyperbolic tangent layer (with 50% dropout). Finally, that outputs to a softmax of the nClasses. We'll predict based on the argmax of the softmax layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layers = [ \n",
    "          Affine(nout=4, init=init, bias=init, activation=Rectlin()), # Affine layer with 4 neurons (ReLU activation)\n",
    "          Affine(nout=8, init=init, bias=init, activation=Tanh()), # Affine layer with 4 neurons (Tanh activation)\n",
    "          Dropout(0.5),  # Dropout layer\n",
    "          Affine(nout=nClasses, init=init, bias=init, activation=Softmax()) # Affine layer with softmax\n",
    "         ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mlp = Model(layers=layers) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost function\n",
    "\n",
    "How \"close\" is the model's prediction is to the true value? For the case of multi-class prediction we typically use [Cross Entropy](https://en.wikipedia.org/wiki/Cross_entropy). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cost = GeneralizedCost(costfunc=CrossEntropyMulti()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent\n",
    "\n",
    "All of our models will use [gradient descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent). We will iteratively update the model weights and biases in order to minimize the cost of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = GradientDescentMomentum(0.1, momentum_coef=0.2) \n",
    "\n",
    "callbacks = Callbacks(mlp, eval_set=training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the model\n",
    "\n",
    "This starts gradient descent. The number of epochs is how many times we want to perform gradient descent on our entire training dataset. So 100 epochs means that we repeat gradient descent on our data 100 times in a row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0   [Train |████████████████████|   10/10   batches, 1.17 cost, 0.03s]\n",
      "Epoch 1   [Train |████████████████████|   10/10   batches, 1.14 cost, 0.03s]\n",
      "Epoch 2   [Train |████████████████████|   10/10   batches, 1.05 cost, 0.03s]\n",
      "Epoch 3   [Train |████████████████████|   10/10   batches, 1.01 cost, 0.03s]\n",
      "Epoch 4   [Train |████████████████████|   10/10   batches, 0.99 cost, 0.03s]\n",
      "Epoch 5   [Train |████████████████████|   10/10   batches, 0.90 cost, 0.03s]\n",
      "Epoch 6   [Train |████████████████████|   10/10   batches, 0.85 cost, 0.03s]\n",
      "Epoch 7   [Train |████████████████████|   10/10   batches, 0.79 cost, 0.02s]\n",
      "Epoch 8   [Train |████████████████████|   10/10   batches, 0.76 cost, 0.03s]\n",
      "Epoch 9   [Train |████████████████████|   10/10   batches, 0.67 cost, 0.03s]\n",
      "Epoch 10  [Train |████████████████████|   10/10   batches, 0.68 cost, 0.04s]\n",
      "Epoch 11  [Train |████████████████████|   10/10   batches, 0.63 cost, 0.04s]\n",
      "Epoch 12  [Train |████████████████████|   10/10   batches, 0.60 cost, 0.04s]\n",
      "Epoch 13  [Train |████████████████████|   10/10   batches, 0.56 cost, 0.03s]\n",
      "Epoch 14  [Train |████████████████████|   10/10   batches, 0.51 cost, 0.04s]\n",
      "Epoch 15  [Train |████████████████████|   10/10   batches, 0.44 cost, 0.03s]\n",
      "Epoch 16  [Train |████████████████████|   10/10   batches, 0.46 cost, 0.03s]\n",
      "Epoch 17  [Train |████████████████████|   10/10   batches, 0.42 cost, 0.03s]\n",
      "Epoch 18  [Train |████████████████████|   10/10   batches, 0.41 cost, 0.03s]\n",
      "Epoch 19  [Train |████████████████████|   10/10   batches, 0.40 cost, 0.03s]\n",
      "Epoch 20  [Train |████████████████████|   10/10   batches, 0.35 cost, 0.03s]\n",
      "Epoch 21  [Train |████████████████████|   10/10   batches, 0.33 cost, 0.03s]\n",
      "Epoch 22  [Train |████████████████████|   10/10   batches, 0.36 cost, 0.03s]\n",
      "Epoch 23  [Train |████████████████████|   10/10   batches, 0.31 cost, 0.03s]\n",
      "Epoch 24  [Train |████████████████████|   10/10   batches, 0.32 cost, 0.03s]\n",
      "Epoch 25  [Train |████████████████████|   10/10   batches, 0.28 cost, 0.03s]\n",
      "Epoch 26  [Train |████████████████████|   10/10   batches, 0.26 cost, 0.03s]\n",
      "Epoch 27  [Train |████████████████████|   10/10   batches, 0.28 cost, 0.03s]\n",
      "Epoch 28  [Train |████████████████████|   10/10   batches, 0.26 cost, 0.03s]\n",
      "Epoch 29  [Train |████████████████████|   10/10   batches, 0.28 cost, 0.03s]\n",
      "Epoch 30  [Train |████████████████████|   10/10   batches, 0.23 cost, 0.03s]\n",
      "Epoch 31  [Train |████████████████████|   10/10   batches, 0.25 cost, 0.03s]\n",
      "Epoch 32  [Train |████████████████████|   10/10   batches, 0.22 cost, 0.03s]\n",
      "Epoch 33  [Train |████████████████████|   10/10   batches, 0.21 cost, 0.03s]\n",
      "Epoch 34  [Train |████████████████████|   10/10   batches, 0.21 cost, 0.03s]\n",
      "Epoch 35  [Train |████████████████████|   10/10   batches, 0.21 cost, 0.03s]\n",
      "Epoch 36  [Train |████████████████████|   10/10   batches, 0.21 cost, 0.03s]\n",
      "Epoch 37  [Train |████████████████████|   10/10   batches, 0.20 cost, 0.03s]\n",
      "Epoch 38  [Train |████████████████████|   10/10   batches, 0.22 cost, 0.03s]\n",
      "Epoch 39  [Train |████████████████████|   10/10   batches, 0.25 cost, 0.03s]\n",
      "Epoch 40  [Train |████████████████████|   10/10   batches, 0.22 cost, 0.03s]\n",
      "Epoch 41  [Train |████████████████████|   10/10   batches, 0.19 cost, 0.03s]\n",
      "Epoch 42  [Train |████████████████████|   10/10   batches, 0.20 cost, 0.03s]\n",
      "Epoch 43  [Train |████████████████████|   10/10   batches, 0.19 cost, 0.03s]\n",
      "Epoch 44  [Train |████████████████████|   10/10   batches, 0.21 cost, 0.03s]\n",
      "Epoch 45  [Train |████████████████████|   10/10   batches, 0.18 cost, 0.03s]\n",
      "Epoch 46  [Train |████████████████████|   10/10   batches, 0.18 cost, 0.03s]\n",
      "Epoch 47  [Train |████████████████████|   10/10   batches, 0.20 cost, 0.03s]\n",
      "Epoch 48  [Train |████████████████████|   10/10   batches, 0.15 cost, 0.03s]\n",
      "Epoch 49  [Train |████████████████████|   10/10   batches, 0.19 cost, 0.03s]\n",
      "Epoch 50  [Train |████████████████████|   10/10   batches, 0.19 cost, 0.03s]\n",
      "Epoch 51  [Train |████████████████████|   10/10   batches, 0.19 cost, 0.03s]\n",
      "Epoch 52  [Train |████████████████████|   10/10   batches, 0.14 cost, 0.03s]\n",
      "Epoch 53  [Train |████████████████████|   10/10   batches, 0.19 cost, 0.03s]\n",
      "Epoch 54  [Train |████████████████████|   10/10   batches, 0.16 cost, 0.03s]\n",
      "Epoch 55  [Train |████████████████████|   10/10   batches, 0.13 cost, 0.03s]\n",
      "Epoch 56  [Train |████████████████████|   10/10   batches, 0.13 cost, 0.03s]\n",
      "Epoch 57  [Train |████████████████████|   10/10   batches, 0.14 cost, 0.03s]\n",
      "Epoch 58  [Train |████████████████████|   10/10   batches, 0.13 cost, 0.03s]\n",
      "Epoch 59  [Train |████████████████████|   10/10   batches, 0.14 cost, 0.03s]\n",
      "Epoch 60  [Train |████████████████████|   10/10   batches, 0.16 cost, 0.03s]\n",
      "Epoch 61  [Train |████████████████████|   10/10   batches, 0.16 cost, 0.03s]\n",
      "Epoch 62  [Train |████████████████████|   10/10   batches, 0.13 cost, 0.03s]\n",
      "Epoch 63  [Train |████████████████████|   10/10   batches, 0.15 cost, 0.03s]\n",
      "Epoch 64  [Train |████████████████████|   10/10   batches, 0.17 cost, 0.03s]\n",
      "Epoch 65  [Train |████████████████████|   10/10   batches, 0.17 cost, 0.03s]\n",
      "Epoch 66  [Train |████████████████████|   10/10   batches, 0.13 cost, 0.03s]\n",
      "Epoch 67  [Train |████████████████████|   10/10   batches, 0.16 cost, 0.03s]\n",
      "Epoch 68  [Train |████████████████████|   10/10   batches, 0.15 cost, 0.03s]\n",
      "Epoch 69  [Train |████████████████████|   10/10   batches, 0.14 cost, 0.03s]\n",
      "Epoch 70  [Train |████████████████████|   10/10   batches, 0.14 cost, 0.03s]\n",
      "Epoch 71  [Train |████████████████████|   10/10   batches, 0.10 cost, 0.03s]\n",
      "Epoch 72  [Train |████████████████████|   10/10   batches, 0.12 cost, 0.03s]\n",
      "Epoch 73  [Train |████████████████████|   10/10   batches, 0.13 cost, 0.03s]\n",
      "Epoch 74  [Train |████████████████████|   10/10   batches, 0.10 cost, 0.03s]\n",
      "Epoch 75  [Train |████████████████████|   10/10   batches, 0.15 cost, 0.03s]\n",
      "Epoch 76  [Train |████████████████████|   10/10   batches, 0.09 cost, 0.03s]\n",
      "Epoch 77  [Train |████████████████████|   10/10   batches, 0.12 cost, 0.03s]\n",
      "Epoch 78  [Train |████████████████████|   10/10   batches, 0.13 cost, 0.03s]\n",
      "Epoch 79  [Train |████████████████████|   10/10   batches, 0.11 cost, 0.03s]\n",
      "Epoch 80  [Train |████████████████████|   10/10   batches, 0.12 cost, 0.03s]\n",
      "Epoch 81  [Train |████████████████████|   10/10   batches, 0.11 cost, 0.03s]\n",
      "Epoch 82  [Train |████████████████████|   10/10   batches, 0.13 cost, 0.03s]\n",
      "Epoch 83  [Train |████████████████████|   10/10   batches, 0.13 cost, 0.03s]\n",
      "Epoch 84  [Train |████████████████████|   10/10   batches, 0.12 cost, 0.03s]\n",
      "Epoch 85  [Train |████████████████████|   10/10   batches, 0.15 cost, 0.03s]\n",
      "Epoch 86  [Train |████████████████████|   10/10   batches, 0.12 cost, 0.03s]\n",
      "Epoch 87  [Train |████████████████████|   10/10   batches, 0.08 cost, 0.03s]\n",
      "Epoch 88  [Train |████████████████████|   10/10   batches, 0.09 cost, 0.03s]\n",
      "Epoch 89  [Train |████████████████████|   10/10   batches, 0.13 cost, 0.03s]\n",
      "Epoch 90  [Train |████████████████████|   10/10   batches, 0.13 cost, 0.03s]\n",
      "Epoch 91  [Train |████████████████████|   10/10   batches, 0.11 cost, 0.03s]\n",
      "Epoch 92  [Train |████████████████████|   10/10   batches, 0.09 cost, 0.03s]\n",
      "Epoch 93  [Train |████████████████████|   10/10   batches, 0.13 cost, 0.03s]\n",
      "Epoch 94  [Train |████████████████████|   10/10   batches, 0.11 cost, 0.03s]\n",
      "Epoch 95  [Train |████████████████████|   10/10   batches, 0.09 cost, 0.03s]\n",
      "Epoch 96  [Train |████████████████████|   10/10   batches, 0.12 cost, 0.03s]\n",
      "Epoch 97  [Train |████████████████████|   10/10   batches, 0.13 cost, 0.03s]\n",
      "Epoch 98  [Train |████████████████████|   10/10   batches, 0.09 cost, 0.03s]\n",
      "Epoch 99  [Train |████████████████████|   10/10   batches, 0.13 cost, 0.03s]\n"
     ]
    }
   ],
   "source": [
    "mlp.fit(training_data, optimizer=optimizer, num_epochs=100, cost=cost, callbacks=callbacks) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the model on the testing data\n",
    "\n",
    "Let's run the model on the testing data and get the predictions. We can then compare those predictions with the true values to see how well our model has performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model misclassified 2.0% of the test data.\n"
     ]
    }
   ],
   "source": [
    "results = mlp.get_outputs(testing_data) \n",
    "prediction = results.argmax(1) \n",
    "\n",
    "error_pct = 100 * mlp.eval(testing_data, metric=Misclassification())[0]\n",
    "print ('The model misclassified {:.1f}% of the test data.'.format(error_pct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:neon]",
   "language": "python",
   "name": "conda-env-neon-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
