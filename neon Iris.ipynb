{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using sklearn's Iris Dataset with neon\n",
    "\n",
    "Tony Reina<br>\n",
    "28 JUNE 2017\n",
    "\n",
    "Here's an example of how we can load one of the standard [sklearn](http://scikit-learn.org/stable/index.html) datasets into a neon model. We'll be using the [iris dataset](http://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html), a classification model which tries to predict the type of iris flower species (Setosa, Versicolour, and Virginica) based on 4 continuous parameters: Sepal Length, Sepal Width, Petal Length and Petal Width. It is based on Ronald Fisher's 1936 paper describing [Linear Discriminant Analysis](https://en.wikipedia.org/wiki/Iris_flower_data_set). The dataset is now considered one of the gold standards at monitoring the performance of a new classification method.\n",
    "\n",
    "In this notebook, we'll walk through loading the data from sklearn into neon's ArrayIterator class and then passing that to a simple multi-layer perceptron model. We should get a misclassification rate of 2% to 8%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
    "\n",
    ">     http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "> Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the iris dataset from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris.data  \n",
    "Y = iris.target\n",
    "\n",
    "nClasses = len(iris.target_names)  # Setosa, Versicolour, and Virginica iris species"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use sklearn to split the data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33) # 66% training, 33% testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make sure that the features are scaled to mean of 0 and standard deviation of 1\n",
    "\n",
    "This is standard pre-processing for multi-layered perceptron inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scl = StandardScaler()\n",
    "\n",
    "X_train = scl.fit_transform(X_train)\n",
    "X_test = scl.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a backend for neon to use\n",
    "\n",
    "This sets up either our GPU or CPU connection to neon. If we don't start with this, then ArrayIterator won't execute.\n",
    "\n",
    "We're asking neon to use the cpu, but can change that to a gpu if it is available. Batch size refers to how many data points are taken at a time. Here's a primer on [Gradient Descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent).\n",
    "\n",
    ">Technical note:  Your batch size must always be much less than the number of points in your data. So if you have 50 points, then set your batch size to something much less than 50. I'd suggest setting the batch size to no more than 10% of the number of data points. You can always just set your batch size to 1. In that case, you are no longer performing mini-batch gradient descent, but are performing the standard stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from neon.data import ArrayIterator\n",
    "from neon.backends import gen_backend\n",
    "\n",
    "be = gen_backend(backend='cpu', batch_size=X_train.shape[0]//10)  # Change to 'gpu' if you have gpu support "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's pass the data to neon\n",
    "\n",
    "We pass our data (both features and labels) into neon's ArrayIterator class.  By default, ArrayIterator one-hot encodes the labels (which saves us a step). Once we get our ArrayIterators, then we can pass them directly into neon models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_data = ArrayIterator(X=X_train, y=y_train, nclass=nClasses, make_onehot=True)\n",
    "testing_data = ArrayIterator(X=X_test, y=y_test, nclass=nClasses, make_onehot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am using this backend: <neon.backends.nervanacpu.NervanaCPU object at 0x7f0b6ff35410>\n"
     ]
    }
   ],
   "source": [
    "print ('I am using this backend: {}'.format(be))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the neon libraries we need for this MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from neon.initializers import GlorotUniform, Gaussian \n",
    "from neon.layers import GeneralizedCost, Affine, Dropout\n",
    "from neon.models import Model \n",
    "from neon.optimizers import GradientDescentMomentum, Adam\n",
    "from neon.transforms import Softmax, CrossEntropyMulti, Rectlin, Tanh\n",
    "from neon.callbacks.callbacks import Callbacks, EarlyStopCallback\n",
    "from neon.transforms import Misclassification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the weights and bias variables\n",
    "\n",
    "We could use numbers from the Gaussian distribution ($\\mu=0, \\sigma=0.3$) to initialize the weights and bias terms for our regression model. However, we can also use other initializations like [GlorotUniform](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = GlorotUniform()    #Gaussian(loc=0, scale=0.3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a multi-layered perceptron (MLP) model\n",
    "\n",
    "We just use a simple Python list to add our different layers to the model. The nice thing is that we've already put our data into a neon ArrayIterator. That means the model will automatically know how to handle the input layer.\n",
    "\n",
    "In this model, the input layer feeds into a 5-neuron rectified linear unit affine layer. That feeds into an 8 neuron hyperbolic tangent layer (with 50% dropout). Finally, that outputs to a softmax of the nClasses. We'll predict based on the argmax of the softmax layer.\n",
    "\n",
    "I've just thrown together a model haphazardly. There is no reason the model has to be like this. In fact, I would suggest playing with adding different layers, different # of neurons, and different activation functions to see if you can get a better model. What's nice about neon is that we can easily alter the model architecture without much change to our code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layers = [ \n",
    "          Affine(nout=5, init=init, bias=init, activation=Rectlin()), # Affine layer with 5 neurons (ReLU activation)\n",
    "          Affine(nout=8, init=init, bias=init, activation=Tanh()), # Affine layer with 7 neurons (Tanh activation)\n",
    "          Dropout(0.5),  # Dropout layer\n",
    "          Affine(nout=nClasses, init=init, bias=init, activation=Softmax()) # Affine layer with softmax\n",
    "         ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mlp = Model(layers=layers) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost function\n",
    "\n",
    "How \"close\" is the model's prediction is to the true value? For the case of multi-class prediction we typically use [Cross Entropy](https://en.wikipedia.org/wiki/Cross_entropy). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cost = GeneralizedCost(costfunc=CrossEntropyMulti()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent\n",
    "\n",
    "All of our models will use [gradient descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent). We will iteratively update the model weights and biases in order to minimize the cost of the model.\n",
    "\n",
    "There are many optimizing algorithms we can use for gradient descent. Here we'll use [Adam](https://neon.nervanasys.com/index.html/optimizers.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#optimizer = GradientDescentMomentum(0.1, momentum_coef=0.2) \n",
    "\n",
    "optimizer = Adam(learning_rate=0.1, beta_1=0.9, beta_2=0.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks\n",
    "\n",
    "[Callbacks](https://neon.nervanasys.com/index.html/callbacks.html) allow us to run custom code at certain points during the training. For example, in the code below we want to find out how well the model is performing against the testing data after every 2 callbacks of training. If the cross entropy error goes up, then we stop the training early. Otherwise, we might be overfitting the model to the training set.\n",
    "\n",
    "I've added a *patience* parameter to the early stopping. If the model's performance has not improved after a certain number of callbacks, then we will stop training early."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define stopping function\n",
    "# it takes as input a tuple (State,val[t])\n",
    "# which describes the cumulative validation state (generated by this function)\n",
    "# and the validation error at time t\n",
    "# and returns as output a tuple (State', Bool),\n",
    "# which represents the new state and whether to stop\n",
    "\n",
    "def stop_func(s, v):\n",
    "    \n",
    "    patience = 4  # If model performance has not improved in this many callbacks, then early stop.\n",
    "    \n",
    "    if s is None:\n",
    "        return ([v], False)\n",
    "    \n",
    "    if (all(v < i for i in s)):  # Check to see if this value is smaller than any in the history\n",
    "        history = [v]  # New value is smaller so let's reset the history\n",
    "        print('Model improved performance: {}'.format(v))\n",
    "    else:\n",
    "        history = s + [v]   # New value is not smaller, so let's add to current history\n",
    "        print('Model has not improved in {} callbacks.'.format(len(history)-1))\n",
    "            \n",
    "    if len(history) > patience:  # If our history is greater than the patience, then early terminate.\n",
    "        stop = True\n",
    "        print('Stopping training early.')\n",
    "    else:\n",
    "        stop = False   # Otherwise, keep training.\n",
    "    \n",
    "    return (history, stop)   \n",
    "\n",
    "# The model trains on the training set, but every 2 epochs we calculate\n",
    "# its performance against the testing set. If the performance increases, then\n",
    "# we want to stop early because we are overfitting our model.\n",
    "callbacks = Callbacks(mlp, eval_set=testing_data, eval_freq=2)  # Run the callback every 2 epochs\n",
    "callbacks.add_callback(EarlyStopCallback(stop_func)) # Add our early stopping function call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the model\n",
    "\n",
    "This starts gradient descent. The number of epochs is how many times we want to perform gradient descent on our entire training dataset. So 100 epochs means that we repeat gradient descent on our data 100 times in a row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0   [Train |████████████████████|   10/10   batches, 0.63 cost, 0.04s]\n",
      "Epoch 1   [Train |████████████████████|   10/10   batches, 0.33 cost, 0.03s] [CrossEntropyMulti Loss 0.31, 0.01s]\n",
      "Epoch 2   [Train |████████████████████|   10/10   batches, 0.30 cost, 0.03s]\n",
      "Epoch 3   [Train |████████████████████|   10/10   batches, 0.23 cost, 0.04s] [CrossEntropyMulti Loss 0.13, 0.00s]\n",
      "Model improved performance: 0.134086459875\n",
      "Epoch 4   [Train |████████████████████|   10/10   batches, 0.24 cost, 0.03s]\n",
      "Epoch 5   [Train |████████████████████|   10/10   batches, 0.18 cost, 0.03s] [CrossEntropyMulti Loss 0.15, 0.00s]\n",
      "Model has not improved in 1 callbacks.\n",
      "Epoch 6   [Train |████████████████████|   10/10   batches, 0.14 cost, 0.03s]\n",
      "Epoch 7   [Train |████████████████████|   10/10   batches, 0.23 cost, 0.03s] [CrossEntropyMulti Loss 0.11, 0.00s]\n",
      "Model improved performance: 0.108739070594\n",
      "Epoch 8   [Train |████████████████████|   10/10   batches, 0.34 cost, 0.03s]\n",
      "Epoch 9   [Train |████████████████████|   10/10   batches, 0.22 cost, 0.03s] [CrossEntropyMulti Loss 0.17, 0.00s]\n",
      "Model has not improved in 1 callbacks.\n",
      "Epoch 10  [Train |████████████████████|   10/10   batches, 0.24 cost, 0.03s]\n",
      "Epoch 11  [Train |████████████████████|   10/10   batches, 0.20 cost, 0.03s] [CrossEntropyMulti Loss 0.15, 0.00s]\n",
      "Model has not improved in 2 callbacks.\n",
      "Epoch 12  [Train |████████████████████|   10/10   batches, 0.13 cost, 0.03s]\n",
      "Epoch 13  [Train |████████████████████|   10/10   batches, 0.23 cost, 0.04s] [CrossEntropyMulti Loss 0.15, 0.00s]\n",
      "Model has not improved in 3 callbacks.\n",
      "Epoch 14  [Train |████████████████████|   10/10   batches, 0.08 cost, 0.03s]\n",
      "Epoch 15  [Train |████████████████████|   10/10   batches, 0.08 cost, 0.03s] [CrossEntropyMulti Loss 0.14, 0.00s]\n",
      "Model has not improved in 4 callbacks.\n",
      "Stopping training early.\n"
     ]
    }
   ],
   "source": [
    "mlp.fit(training_data, optimizer=optimizer, num_epochs=100, cost=cost, callbacks=callbacks) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the model on the testing data\n",
    "\n",
    "Let's run the model on the testing data and get the predictions. We can then compare those predictions with the true values to see how well our model has performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model misclassified 6.00% of the test data.\n"
     ]
    }
   ],
   "source": [
    "results = mlp.get_outputs(testing_data) \n",
    "prediction = results.argmax(1) \n",
    "\n",
    "error_pct = 100 * mlp.eval(testing_data, metric=Misclassification())[0]\n",
    "print ('The model misclassified {:.2f}% of the test data.'.format(error_pct))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Save the model\n",
    "\n",
    "Let's save the model and the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mlp.save_params('iris_model.prm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here's the text description of the model.\n",
    "\n",
    "You could use this to draw a graph of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'config': {'layers': [{'config': {'init': {'config': {},\n",
       "      'type': 'neon.initializers.initializer.GlorotUniform'},\n",
       "     'name': 'Linear_0',\n",
       "     'nout': 5},\n",
       "    'type': 'neon.layers.layer.Linear'},\n",
       "   {'config': {'init': {'config': {},\n",
       "      'type': 'neon.initializers.initializer.GlorotUniform'},\n",
       "     'name': 'Linear_0_bias'},\n",
       "    'type': 'neon.layers.layer.Bias'},\n",
       "   {'config': {'name': 'Linear_0_Rectlin',\n",
       "     'transform': {'config': {'name': 'Rectlin_0'},\n",
       "      'type': 'neon.transforms.activation.Rectlin'}},\n",
       "    'type': 'neon.layers.layer.Activation'},\n",
       "   {'config': {'init': {'config': {},\n",
       "      'type': 'neon.initializers.initializer.GlorotUniform'},\n",
       "     'name': 'Linear_1',\n",
       "     'nout': 8},\n",
       "    'type': 'neon.layers.layer.Linear'},\n",
       "   {'config': {'init': {'config': {},\n",
       "      'type': 'neon.initializers.initializer.GlorotUniform'},\n",
       "     'name': 'Linear_1_bias'},\n",
       "    'type': 'neon.layers.layer.Bias'},\n",
       "   {'config': {'name': 'Linear_1_Tanh',\n",
       "     'transform': {'config': {'name': 'Tanh_0'},\n",
       "      'type': 'neon.transforms.activation.Tanh'}},\n",
       "    'type': 'neon.layers.layer.Activation'},\n",
       "   {'config': {'name': 'Dropout_0'}, 'type': 'neon.layers.layer.Dropout'},\n",
       "   {'config': {'init': {'config': {},\n",
       "      'type': 'neon.initializers.initializer.GlorotUniform'},\n",
       "     'name': 'Linear_2',\n",
       "     'nout': 3},\n",
       "    'type': 'neon.layers.layer.Linear'},\n",
       "   {'config': {'init': {'config': {},\n",
       "      'type': 'neon.initializers.initializer.GlorotUniform'},\n",
       "     'name': 'Linear_2_bias'},\n",
       "    'type': 'neon.layers.layer.Bias'},\n",
       "   {'config': {'name': 'Linear_2_Softmax',\n",
       "     'transform': {'config': {'name': 'Softmax_0'},\n",
       "      'type': 'neon.transforms.activation.Softmax'}},\n",
       "    'type': 'neon.layers.layer.Activation'}],\n",
       "  'name': 'Sequential_0'},\n",
       " 'container': True,\n",
       " 'type': 'neon.layers.container.Sequential'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.get_description()['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:neon]",
   "language": "python",
   "name": "conda-env-neon-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
